{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Libs ############################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction\n",
    "Loads a dataset with product data from a specified path available in the environment variable DATASET_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>query</th>\n",
       "      <th>search_page</th>\n",
       "      <th>position</th>\n",
       "      <th>title</th>\n",
       "      <th>concatenated_tags</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>price</th>\n",
       "      <th>weight</th>\n",
       "      <th>express_delivery</th>\n",
       "      <th>minimum_quantity</th>\n",
       "      <th>view_counts</th>\n",
       "      <th>order_counts</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11394449</td>\n",
       "      <td>8324141</td>\n",
       "      <td>espirito santo</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Mandala Espírito Santo</td>\n",
       "      <td>mandala mdf</td>\n",
       "      <td>2015-11-14 19:42:12</td>\n",
       "      <td>171.89</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Decoração</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15534262</td>\n",
       "      <td>6939286</td>\n",
       "      <td>cartao de visita</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Cartão de Visita</td>\n",
       "      <td>cartao visita panfletos tag adesivos copos lon...</td>\n",
       "      <td>2018-04-04 20:55:07</td>\n",
       "      <td>77.67</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Papel e Cia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  seller_id             query  search_page  position  \\\n",
       "0    11394449    8324141    espirito santo            2         6   \n",
       "1    15534262    6939286  cartao de visita            2         0   \n",
       "\n",
       "                    title                                  concatenated_tags  \\\n",
       "0  Mandala Espírito Santo                                        mandala mdf   \n",
       "1        Cartão de Visita  cartao visita panfletos tag adesivos copos lon...   \n",
       "\n",
       "         creation_date   price  weight  express_delivery  minimum_quantity  \\\n",
       "0  2015-11-14 19:42:12  171.89  1200.0                 1                 4   \n",
       "1  2018-04-04 20:55:07   77.67     8.0                 1                 5   \n",
       "\n",
       "   view_counts  order_counts     category  \n",
       "0          244           NaN    Decoração  \n",
       "1          124           NaN  Papel e Cia  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    DATASET_PATH = os.environ['DATASET_PATH']\n",
    "except:\n",
    "    DATASET_PATH = 'data/sample_products.csv'\n",
    "\n",
    "df_file = pd.read_csv(DATASET_PATH, sep=',')\n",
    "df_file.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data formatting\n",
    "Processes the dataset to use it for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_file.copy()\n",
    "df_data.isnull().sum()\n",
    "import re\n",
    "import unicodedata\n",
    "def dedup_esp_caract(txt):\n",
    "    x = str(txt)\n",
    "    nfkd = unicodedata.normalize('NFKD', x)\n",
    "    palavraSemAcento = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "    x = re.sub('[^a-zA-Z0-9 \\\\\\]','', palavraSemAcento)\n",
    "    x = x.split(' ')\n",
    "    x = ' '.join([str(elem) for elem in set(x)])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoder, using only text columns\n",
    "LE = LabelEncoder()\n",
    "df_data['cod_category'] = LE.fit_transform(df_data['category'])\n",
    "df_data['text_train'] = df_data['query'].str.lower() +' ' + df_data['title'].str.lower() + ' ' + df_data['concatenated_tags'].str.lower()\n",
    "df_data['text_train'] = df_data.apply(lambda row: dedup_esp_caract(row['text_train']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Specifies a model to handle the categorization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25460,)\n",
      "(12540,)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df_data, random_state=42, test_size=0.33, shuffle=True)\n",
    "X_train = train.text_train\n",
    "X_test = test.text_train\n",
    "y_train = train['cod_category']\n",
    "y_test = test['cod_category']\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86      2283\n",
      "           1       0.97      0.89      0.93       297\n",
      "           2       0.88      0.91      0.89      2856\n",
      "           3       0.87      0.95      0.91      5805\n",
      "           4       0.91      0.49      0.63       367\n",
      "           5       0.85      0.63      0.72       932\n",
      "\n",
      "    accuracy                           0.88     12540\n",
      "   macro avg       0.89      0.78      0.82     12540\n",
      "weighted avg       0.88      0.88      0.88     12540\n",
      "\n",
      "0.8792663476874003\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression #########\n",
    "LR_Pipe = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1))\n",
    "           ])\n",
    "LR_Pipe.fit(X_train.values.astype('U'), y_train)\n",
    "y_pred = LR_Pipe.predict(X_test.values.astype('U'))\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      2283\n",
      "           1       0.96      0.95      0.95       297\n",
      "           2       0.90      0.90      0.90      2856\n",
      "           3       0.88      0.96      0.92      5805\n",
      "           4       0.93      0.58      0.71       367\n",
      "           5       0.89      0.66      0.75       932\n",
      "\n",
      "    accuracy                           0.90     12540\n",
      "   macro avg       0.91      0.82      0.85     12540\n",
      "weighted avg       0.90      0.90      0.89     12540\n",
      "\n",
      "0.8964114832535885\n"
     ]
    }
   ],
   "source": [
    "#Random Forest ###############\n",
    "RF_Pipe = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('RF', RandomForestClassifier(n_estimators=100,random_state=2)),\n",
    "           ])\n",
    "\n",
    "\n",
    "RF_Pipe.fit(X_train.values.astype('U'), y_train)\n",
    "y_pred2 = RF_Pipe.predict(X_test.values.astype('U'))\n",
    "\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82      2283\n",
      "           1       0.95      0.82      0.88       297\n",
      "           2       0.65      0.90      0.76      2856\n",
      "           3       0.90      0.84      0.87      5805\n",
      "           4       0.70      0.33      0.45       367\n",
      "           5       0.80      0.55      0.65       932\n",
      "\n",
      "    accuracy                           0.81     12540\n",
      "   macro avg       0.81      0.70      0.74     12540\n",
      "weighted avg       0.82      0.81      0.81     12540\n",
      "\n",
      "0.8064593301435407\n"
     ]
    }
   ],
   "source": [
    "#Decision Treet ###############\n",
    "DT_Pipe = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('DT', DecisionTreeClassifier(criterion=\"entropy\",max_depth=60)),\n",
    "           ])\n",
    "\n",
    "\n",
    "DT_Pipe.fit(X_train.values.astype('U'), y_train)\n",
    "y_pred3 = DT_Pipe.predict(X_test.values.astype('U'))\n",
    "\n",
    "print(classification_report(y_test,y_pred3))\n",
    "print(accuracy_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose best model by acc\n",
    "best_model = []\n",
    "models = {'RF_Pipe' : accuracy_score(y_test, y_pred2),'LR_Pipe' : accuracy_score(y_test, y_pred), 'DT_Pipe' : accuracy_score(y_test, y_pred3)}\n",
    "models = {k: v for k, v in sorted(models.items(), key=lambda item: item[1])}\n",
    "best_model = list(models.keys())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "Generates metrics about the model accuracy (precision, recall, F1, etc.) for each category and exports them to a specified path available in the environment variable METRICS_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    METRICS_PATH = os.environ['METRICS_PATH']\n",
    "except:\n",
    "    METRICS_PATH = 'metrics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorias = df_data[['cod_category', 'category']].copy()\n",
    "df_categorias = df_categorias.drop_duplicates()\n",
    "categories = df_categorias['category'].tolist()\n",
    "\n",
    "def test_predict(Model, Save, txt):    \n",
    "    metric_file =''\n",
    "    metric_file += '####################################\\n'\n",
    "    metric_file += txt + '\\n'\n",
    "    metric_file += '####################################\\n'\n",
    "        \n",
    "    for cat in categories:    \n",
    "        x_cat_test = test['text_train'][test['category']==cat]\n",
    "        y_cat_test = test['cod_category'][test['category']==cat]\n",
    "        y_pred = Model.predict(x_cat_test)\n",
    "        print('ACC: ',cat, accuracy_score(y_cat_test, y_pred))        \n",
    "        metric_file += 'Category: ' + cat + ' - ACC:' + str(accuracy_score(y_cat_test, y_pred))  + '\\n'\n",
    "        metric_file += str(classification_report(y_cat_test,y_pred)) + '\\n'\n",
    "        print(classification_report(y_cat_test,y_pred))\n",
    "    \n",
    "    if Save == 1:                        \n",
    "        f = open(METRICS_PATH, \"a\")        \n",
    "        f.truncate(0) # need '0' when using r+\n",
    "        f.write(metric_file)\n",
    "        f.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Random Forest Metrics for categories #################\n",
      "ACC:  Decoração 0.9005602240896359\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.90      0.95      2856\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.90      2856\n",
      "   macro avg       0.17      0.15      0.16      2856\n",
      "weighted avg       1.00      0.90      0.95      2856\n",
      "\n",
      "ACC:  Papel e Cia 0.6555793991416309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       1.00      0.66      0.79       932\n",
      "\n",
      "    accuracy                           0.66       932\n",
      "   macro avg       0.17      0.11      0.13       932\n",
      "weighted avg       1.00      0.66      0.79       932\n",
      "\n",
      "ACC:  Outros 0.5776566757493188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.58      0.73       367\n",
      "           5       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.58       367\n",
      "   macro avg       0.17      0.10      0.12       367\n",
      "weighted avg       1.00      0.58      0.73       367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:  Bebê 0.8611476127901884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.93      2283\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.86      2283\n",
      "   macro avg       0.25      0.22      0.23      2283\n",
      "weighted avg       1.00      0.86      0.93      2283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:  Lembrancinhas 0.9645133505598622\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       1.00      0.96      0.98      5805\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96      5805\n",
      "   macro avg       0.17      0.16      0.16      5805\n",
      "weighted avg       1.00      0.96      0.98      5805\n",
      "\n",
      "ACC:  Bijuterias e Jóias 0.9461279461279462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.95      0.97       297\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.95       297\n",
      "   macro avg       0.25      0.24      0.24       297\n",
      "weighted avg       1.00      0.95      0.97       297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "if best_model == 'RF_Pipe':\n",
    "    print('############### Random Forest Metrics for categories #################')\n",
    "    test_predict(RF_Pipe, 1, 'Random Forest - Metrics for categories')\n",
    "elif best_model == 'DT_Pipe':\n",
    "    print('############### Logistic Regression Metrics for categories #################')\n",
    "    test_predict(LR_Pipe, 1, 'Decision Tree - Metrics for categories')\n",
    "elif best_model == 'LR_Pipe':\n",
    "    print('############### Logistic Regression Metrics for categories #################')\n",
    "    test_predict(LR_Pipe, 1, 'Logistic Regression - Metrics for categories')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model exportation\n",
    "Exports a candidate model to a specified path available in the environment variable MODEL_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest\n",
    "def save_model(model):\n",
    "    try:\n",
    "        MODEL_PATH = os.environ['MODEL_PATH']\n",
    "    except:\n",
    "        MODEL_PATH = 'model.pkl'\n",
    "    with open(MODEL_PATH, 'wb') as picklefile:\n",
    "        pickle.dump(model,picklefile)\n",
    "    \n",
    "#Save Best model    \n",
    "if best_model == 'RF_Pipe':\n",
    "    save_model(RF_Pipe)\n",
    "elif best_model == 'DT_Pipe':\n",
    "    save_model(LR_Pipe)\n",
    "elif best_model == 'LR_Pipe':\n",
    "    save_model(LR_Pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
