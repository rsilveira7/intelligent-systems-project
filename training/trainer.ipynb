{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Libs ############################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction\n",
    "Loads a dataset with product data from a specified path available in the environment variable DATASET_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>query</th>\n",
       "      <th>search_page</th>\n",
       "      <th>position</th>\n",
       "      <th>title</th>\n",
       "      <th>concatenated_tags</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>price</th>\n",
       "      <th>weight</th>\n",
       "      <th>express_delivery</th>\n",
       "      <th>minimum_quantity</th>\n",
       "      <th>view_counts</th>\n",
       "      <th>order_counts</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11394449</td>\n",
       "      <td>8324141</td>\n",
       "      <td>espirito santo</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Mandala Espírito Santo</td>\n",
       "      <td>mandala mdf</td>\n",
       "      <td>2015-11-14 19:42:12</td>\n",
       "      <td>171.89</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Decoração</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15534262</td>\n",
       "      <td>6939286</td>\n",
       "      <td>cartao de visita</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Cartão de Visita</td>\n",
       "      <td>cartao visita panfletos tag adesivos copos lon...</td>\n",
       "      <td>2018-04-04 20:55:07</td>\n",
       "      <td>77.67</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Papel e Cia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  seller_id             query  search_page  position  \\\n",
       "0    11394449    8324141    espirito santo            2         6   \n",
       "1    15534262    6939286  cartao de visita            2         0   \n",
       "\n",
       "                    title                                  concatenated_tags  \\\n",
       "0  Mandala Espírito Santo                                        mandala mdf   \n",
       "1        Cartão de Visita  cartao visita panfletos tag adesivos copos lon...   \n",
       "\n",
       "         creation_date   price  weight  express_delivery  minimum_quantity  \\\n",
       "0  2015-11-14 19:42:12  171.89  1200.0                 1                 4   \n",
       "1  2018-04-04 20:55:07   77.67     8.0                 1                 5   \n",
       "\n",
       "   view_counts  order_counts     category  \n",
       "0          244           NaN    Decoração  \n",
       "1          124           NaN  Papel e Cia  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    DATASET_PATH = os.environ['DATASET_PATH']\n",
    "except:\n",
    "    DATASET_PATH = 'data/sample_products.csv'\n",
    "\n",
    "df_file = pd.read_csv(DATASET_PATH, sep=',')\n",
    "df_file.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Bebê                   6930\n",
       "Bijuterias e Jóias      940\n",
       "Decoração              8723\n",
       "Lembrancinhas         17524\n",
       "Outros                 1133\n",
       "Papel e Cia            2750\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_file.groupby(['category']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data formatting\n",
    "Processes the dataset to use it for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy dataset\n",
    "df_data = df_file.copy()\n",
    "df_data.isnull().sum()\n",
    "\n",
    "\n",
    "#remove duplicate itens and especial caracters\n",
    "def dedup_esp_caract(txt):\n",
    "    x = str(txt)\n",
    "    nfkd = unicodedata.normalize('NFKD', x)\n",
    "    palavraSemAcento = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "    x = re.sub('[^a-zA-Z0-9 \\\\\\]','', palavraSemAcento)\n",
    "    x = x.split(' ')\n",
    "    x = ' '.join([str(elem) for elem in set(x)])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Aug for Outros category\n",
    "df_tmp = df_file[df_file['category']=='Outros']\n",
    "df_tmp = df_tmp.loc[df_tmp.index.repeat(3)]\n",
    "df_data = df_data.append(df_tmp)\n",
    "\n",
    "df_tmp = df_file[df_file['category']=='Papel e Cia']\n",
    "df_tmp = df_tmp.loc[df_tmp.index.repeat(2)]\n",
    "df_data = df_data.append(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Bebê                   6930\n",
       "Bijuterias e Jóias      940\n",
       "Decoração              8723\n",
       "Lembrancinhas         17524\n",
       "Outros                 4532\n",
       "Papel e Cia            8250\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.groupby(['category']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoder\n",
    "LE = LabelEncoder()\n",
    "df_data['cod_category'] = LE.fit_transform(df_data['category'])\n",
    "\n",
    "#generate a text column combining query + title + concatenated_tags\n",
    "df_data['text_train'] = df_data['query'].str.lower() +' ' + df_data['title'].str.lower() + ' ' + df_data['concatenated_tags'].str.lower()\n",
    "df_data['text_train'] = df_data.apply(lambda row: dedup_esp_caract(row['text_train']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Specifies a model to handle the categorization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31422,)\n",
      "(15477,)\n"
     ]
    }
   ],
   "source": [
    "#Split dataset into train and test\n",
    "train, test = train_test_split(df_data, random_state=42, test_size=0.33, shuffle=True)\n",
    "X_train = train.text_train\n",
    "X_test = test.text_train\n",
    "y_train = train['cod_category']\n",
    "y_test = test['cod_category']\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.86      2291\n",
      "           1       0.94      0.85      0.89       302\n",
      "           2       0.88      0.89      0.89      2901\n",
      "           3       0.86      0.92      0.89      5776\n",
      "           4       0.92      0.87      0.90      1463\n",
      "           5       0.88      0.83      0.86      2744\n",
      "\n",
      "    accuracy                           0.88     15477\n",
      "   macro avg       0.90      0.87      0.88     15477\n",
      "weighted avg       0.88      0.88      0.88     15477\n",
      "\n",
      "0.8807262389351942\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression #########\n",
    "LR_Pipe = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1))\n",
    "           ])\n",
    "LR_Pipe.fit(X_train.values.astype('U'), y_train)\n",
    "y_pred = LR_Pipe.predict(X_test.values.astype('U'))\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89      2291\n",
      "           1       0.94      0.89      0.92       302\n",
      "           2       0.92      0.90      0.91      2901\n",
      "           3       0.92      0.95      0.94      5776\n",
      "           4       0.97      0.99      0.98      1463\n",
      "           5       0.95      0.98      0.96      2744\n",
      "\n",
      "    accuracy                           0.93     15477\n",
      "   macro avg       0.94      0.93      0.93     15477\n",
      "weighted avg       0.93      0.93      0.93     15477\n",
      "\n",
      "0.9342249790010984\n"
     ]
    }
   ],
   "source": [
    "#Random Forest ###############\n",
    "RF_Pipe = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('RF', RandomForestClassifier(n_estimators=200,random_state=2)),\n",
    "           ])\n",
    "\n",
    "\n",
    "RF_Pipe.fit(X_train.values.astype('U'), y_train)\n",
    "y_pred2 = RF_Pipe.predict(X_test.values.astype('U'))\n",
    "\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.75      0.81      2291\n",
      "           1       0.94      0.74      0.83       302\n",
      "           2       0.57      0.89      0.70      2901\n",
      "           3       0.91      0.83      0.87      5776\n",
      "           4       0.92      0.61      0.73      1463\n",
      "           5       0.90      0.83      0.87      2744\n",
      "\n",
      "    accuracy                           0.81     15477\n",
      "   macro avg       0.86      0.77      0.80     15477\n",
      "weighted avg       0.84      0.81      0.81     15477\n",
      "\n",
      "0.8076500613814047\n"
     ]
    }
   ],
   "source": [
    "#Decision Treet ###############\n",
    "DT_Pipe = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('DT', DecisionTreeClassifier(criterion=\"entropy\",max_depth=60)),\n",
    "           ])\n",
    "\n",
    "\n",
    "DT_Pipe.fit(X_train.values.astype('U'), y_train)\n",
    "y_pred3 = DT_Pipe.predict(X_test.values.astype('U'))\n",
    "\n",
    "print(classification_report(y_test,y_pred3))\n",
    "print(accuracy_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DT_Pipe': 0.8076500613814047, 'LR_Pipe': 0.8807262389351942, 'RF_Pipe': 0.9342249790010984}\n"
     ]
    }
   ],
   "source": [
    "#choose best model by acc\n",
    "best_model = []\n",
    "models = {'RF_Pipe' : accuracy_score(y_test, y_pred2),'LR_Pipe' : accuracy_score(y_test, y_pred), 'DT_Pipe' : accuracy_score(y_test, y_pred3)}\n",
    "models = {k: v for k, v in sorted(models.items(), key=lambda item: item[1])}\n",
    "best_model = list(models.keys())[-1]\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "Generates metrics about the model accuracy (precision, recall, F1, etc.) for each category and exports them to a specified path available in the environment variable METRICS_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    METRICS_PATH = os.environ['METRICS_PATH']\n",
    "except:\n",
    "    METRICS_PATH = 'metrics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a categories list\n",
    "df_categorias = df_data[['cod_category', 'category']].copy()\n",
    "df_categorias = df_categorias.drop_duplicates()\n",
    "categories = df_categorias['category'].tolist()\n",
    "\n",
    "#predict function to each category and save it to file\n",
    "def test_predict(Model, Save, txt):    \n",
    "    metric_file =''\n",
    "    metric_file += '####################################\\n'\n",
    "    metric_file += txt + '\\n'\n",
    "    metric_file += '####################################\\n'\n",
    "        \n",
    "    for cat in categories:    \n",
    "        x_cat_test = test['text_train'][test['category']==cat]\n",
    "        y_cat_test = test['cod_category'][test['category']==cat]\n",
    "        y_pred = Model.predict(x_cat_test)\n",
    "        print('ACC: ',cat, accuracy_score(y_cat_test, y_pred))        \n",
    "        metric_file += 'Category: ' + cat + ' - ACC:' + str(accuracy_score(y_cat_test, y_pred))  + '\\n'\n",
    "        metric_file += str(classification_report(y_cat_test,y_pred)) + '\\n'\n",
    "        print(classification_report(y_cat_test,y_pred))\n",
    "    \n",
    "    if Save == 1:                        \n",
    "        f = open(METRICS_PATH, \"a\")        \n",
    "        f.truncate(0)\n",
    "        f.write(metric_file)\n",
    "        f.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Random Forest Metrics for categories #################\n",
      "ACC:  Decoração 0.9000344708721131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.90      0.95      2901\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.90      2901\n",
      "   macro avg       0.17      0.15      0.16      2901\n",
      "weighted avg       1.00      0.90      0.95      2901\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:  Papel e Cia 0.9752186588921283\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           5       1.00      0.98      0.99      2744\n",
      "\n",
      "    accuracy                           0.98      2744\n",
      "   macro avg       0.25      0.24      0.25      2744\n",
      "weighted avg       1.00      0.98      0.99      2744\n",
      "\n",
      "ACC:  Outros 0.9856459330143541\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.99      0.99      1463\n",
      "\n",
      "    accuracy                           0.99      1463\n",
      "   macro avg       0.25      0.25      0.25      1463\n",
      "weighted avg       1.00      0.99      0.99      1463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:  Bebê 0.8542121344391096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92      2291\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.85      2291\n",
      "   macro avg       0.20      0.17      0.18      2291\n",
      "weighted avg       1.00      0.85      0.92      2291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:  Lembrancinhas 0.9527354570637119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       1.00      0.95      0.98      5776\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.95      5776\n",
      "   macro avg       0.17      0.16      0.16      5776\n",
      "weighted avg       1.00      0.95      0.98      5776\n",
      "\n",
      "ACC:  Bijuterias e Jóias 0.8940397350993378\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.89      0.94       302\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89       302\n",
      "   macro avg       0.20      0.18      0.19       302\n",
      "weighted avg       1.00      0.89      0.94       302\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#choose the best model and call test_predict function\n",
    "if best_model == 'RF_Pipe':\n",
    "    print('############### Random Forest Metrics for categories #################')\n",
    "    test_predict(RF_Pipe, 1, 'Random Forest - Metrics for categories')\n",
    "elif best_model == 'DT_Pipe':\n",
    "    print('############### Logistic Regression Metrics for categories #################')\n",
    "    test_predict(LR_Pipe, 1, 'Decision Tree - Metrics for categories')\n",
    "elif best_model == 'LR_Pipe':\n",
    "    print('############### Logistic Regression Metrics for categories #################')\n",
    "    test_predict(LR_Pipe, 1, 'Logistic Regression - Metrics for categories')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model exportation\n",
    "Exports a candidate model to a specified path available in the environment variable MODEL_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best model as pickle file\n",
    "def save_model(model):\n",
    "    try:\n",
    "        MODEL_PATH = os.environ['MODEL_PATH']\n",
    "    except:\n",
    "        MODEL_PATH = 'model.pkl'\n",
    "    with open(MODEL_PATH, 'wb') as picklefile:\n",
    "        pickle.dump(model,picklefile)\n",
    "    \n",
    "#choose the best model and call save_model function   \n",
    "if best_model == 'RF_Pipe':\n",
    "    save_model(RF_Pipe)\n",
    "elif best_model == 'DT_Pipe':\n",
    "    save_model(LR_Pipe)\n",
    "elif best_model == 'LR_Pipe':\n",
    "    save_model(LR_Pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
